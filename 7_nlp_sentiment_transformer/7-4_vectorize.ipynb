{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 word2vec、fastTextを用いた日本語単語のベクトル表現の実装\n",
    "\n",
    "- 本ファイルでは、日本語の単語をword2vecもしくはfastTextを使用してベクトル化する手法を解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 学習目標\n",
    "\n",
    "1.\t学習済みの日本語word2vecモデルで単語をベクトル表現に変換する実装ができるようになる\n",
    "2.\t学習済みの日本語fastText モデルで単語をベクトル表現に変換する実装ができるようになる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "書籍の指示に従い、本章で使用するデータを用意します\n",
    "\n",
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.7.0->gensim) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 文書を読み込んで、分かち書き、データセット作成まで（8.2と同じです）\n",
    "\n",
    "前処理と分かち書きをし、最後にデータセットを作成する部分を実装します\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語分割にはMecab＋NEologdを使用\n",
    "import MeCab\n",
    "\n",
    "m_t = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def tokenizer_mecab(text):\n",
    "    text = m_t.parse(text)  # これでスペースで単語が区切られる\n",
    "    ret = text.strip().split()  # スペース部分で区切ったリストに変換\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "# 前処理として正規化をする関数を定義\n",
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)  # 数字\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 前処理とJanomeの単語分割を合わせた関数を定義する\n",
    "\n",
    "\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)  # 前処理の正規化\n",
    "    ret = tokenizer_mecab(text)  # Mecabの単語分割\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "# tsvやcsvデータを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "# 文章とラベルの両方に用意します\n",
    "\n",
    "max_length = 25\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
    "                            use_vocab=True, lower=True, include_lengths=True, batch_first=True, fix_length=max_length)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='text_train.tsv',\n",
    "    validation='text_val.tsv', test='text_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 単語のベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語をベクトル表現に変換します。\n",
    "\n",
    "TorchTextには日本語の学習済みデータがないわけではないですが、精度が微妙なので\n",
    "\n",
    "東北大学 乾・岡崎研究室で公開されているWord2Vecの学習済みのベクトルを使用します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下から、日本語のfasttextの学習済みベクトルをダウンロードします\n",
    "\n",
    "# 東北大学 乾・岡崎研究室：日本語 Wikipedia エンティティベクトル\n",
    "\n",
    "# http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/\n",
    "# http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83749/2283665198.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  model.wv.save_word2vec_format('./data/japanese_word2vec_vectors.vec')\n"
     ]
    }
   ],
   "source": [
    "# そのままではtorchtextで読み込めないので、gensimライブラリを使用して、\n",
    "# Word2Vecのformatで保存し直します\n",
    "\n",
    "# 事前インストール\n",
    "# pip install gensim\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# 一度gensimライブラリで読み込んで、word2vecのformatで保存する\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    './data/entity_vector/entity_vector.model.bin', binary=True)\n",
    "\n",
    "# 保存（時間がかかります、10分弱）\n",
    "model.wv.save_word2vec_format('./data/japanese_word2vec_vectors.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1単語を表現する次元数： 200\n",
      "単語数： 1015474\n"
     ]
    }
   ],
   "source": [
    "# torchtextで単語ベクトルとして読み込みます\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_word2vec_vectors = Vectors(\n",
    "    name='./data/japanese_word2vec_vectors.vec')\n",
    "\n",
    "# 単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", japanese_word2vec_vectors.dim)\n",
    "print(\"単語数：\", len(japanese_word2vec_vectors.itos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.6023, -2.6357, -2.5822,  ...,  0.6953, -1.4977,  1.4752],\n",
       "        ...,\n",
       "        [-2.8353,  2.5609, -0.5348,  ...,  0.4602,  1.4669, -2.1255],\n",
       "        [-1.5885,  0.1614, -0.6029,  ..., -1.7545, -1.2462,  2.3034],\n",
       "        [-0.0448, -0.1304,  0.0329,  ...,  0.0825, -0.1386,  0.0417]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds, vectors=japanese_word2vec_vectors, min_freq=1)\n",
    "\n",
    "# ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape)  # 49個の単語が200次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fbf065a0d90>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'と': 2,\n",
       "             '。': 3,\n",
       "             'な': 4,\n",
       "             'の': 5,\n",
       "             '文章': 6,\n",
       "             '、': 7,\n",
       "             'が': 8,\n",
       "             'し': 9,\n",
       "             'を': 10,\n",
       "             'いる': 11,\n",
       "             'か': 12,\n",
       "             'て': 13,\n",
       "             'ます': 14,\n",
       "             '分類': 15,\n",
       "             '本章': 16,\n",
       "             '評価': 17,\n",
       "             '0': 18,\n",
       "             'い': 19,\n",
       "             'から': 20,\n",
       "             'する': 21,\n",
       "             'その': 22,\n",
       "             'た': 23,\n",
       "             'で': 24,\n",
       "             'です': 25,\n",
       "             'に': 26,\n",
       "             'に対して': 27,\n",
       "             'は': 28,\n",
       "             'まし': 29,\n",
       "             'クラス': 30,\n",
       "             'ネガティブ': 31,\n",
       "             'ポジティブ': 32,\n",
       "             'モデル': 33,\n",
       "             'レビュー': 34,\n",
       "             '値': 35,\n",
       "             '取り組み': 36,\n",
       "             '商品': 37,\n",
       "             '女性': 38,\n",
       "             '女王': 39,\n",
       "             '好き': 40,\n",
       "             '姫': 41,\n",
       "             '構築': 42,\n",
       "             '機械学習': 43,\n",
       "             '王': 44,\n",
       "             '王子': 45,\n",
       "             '男性': 46,\n",
       "             '短い': 47,\n",
       "             '自然言語処理': 48})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女王 tensor(0.3840)\n",
      "王 tensor(0.3669)\n",
      "王子 tensor(0.5489)\n",
      "機械学習 tensor(-0.1404)\n"
     ]
    }
   ],
   "source": [
    "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 姫 - 女性 + 男性\n",
    "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
    "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
    "\n",
    "# コサイン類似度を計算\n",
    "# dim=0 は0次元目で計算してくださいという指定\n",
    "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
    "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
    "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
    "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "姫 - 女性 + 男性　を計算すると狙った通り、王子がもっとも近い結果になりました"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vecより進歩したベクトル化手法であるfastTextによる単語のベクトル表現を使用します。\n",
    "\n",
    "日本語の学習モデルを以下の記事にて公開してくださっているので、使用させていただきます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qiita：いますぐ使える単語埋め込みベクトルのリスト\n",
    "# https://qiita.com/Hironsan/items/8f7d35f0a36e0f99752c\n",
    "\n",
    "# Download Word Vectors\n",
    "# https://drive.google.com/open?id=0ByFQ96A4DgSPNFdleG1GaHcxQzA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1単語を表現する次元数： 300\n",
      "単語数： 351122\n"
     ]
    }
   ],
   "source": [
    "# torchtextで単語ベクトルとして読み込みます\n",
    "# word2vecとは異なり、すぐに読み込めます\n",
    "\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "japanese_fasttext_vectors = Vectors(name='./data/vector_neologd/model.vec')\n",
    "\n",
    "                                    \n",
    "# 単語ベクトルの中身を確認します\n",
    "print(\"1単語を表現する次元数：\", japanese_fasttext_vectors.dim)\n",
    "print(\"単語数：\", len(japanese_fasttext_vectors.itos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fbf065a3a90>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'と': 2,\n",
       "             '。': 3,\n",
       "             'な': 4,\n",
       "             'の': 5,\n",
       "             '文章': 6,\n",
       "             '、': 7,\n",
       "             'が': 8,\n",
       "             'し': 9,\n",
       "             'を': 10,\n",
       "             'いる': 11,\n",
       "             'か': 12,\n",
       "             'て': 13,\n",
       "             'ます': 14,\n",
       "             '分類': 15,\n",
       "             '本章': 16,\n",
       "             '評価': 17,\n",
       "             '0': 18,\n",
       "             'い': 19,\n",
       "             'から': 20,\n",
       "             'する': 21,\n",
       "             'その': 22,\n",
       "             'た': 23,\n",
       "             'で': 24,\n",
       "             'です': 25,\n",
       "             'に': 26,\n",
       "             'に対して': 27,\n",
       "             'は': 28,\n",
       "             'まし': 29,\n",
       "             'クラス': 30,\n",
       "             'ネガティブ': 31,\n",
       "             'ポジティブ': 32,\n",
       "             'モデル': 33,\n",
       "             'レビュー': 34,\n",
       "             '値': 35,\n",
       "             '取り組み': 36,\n",
       "             '商品': 37,\n",
       "             '女性': 38,\n",
       "             '女王': 39,\n",
       "             '好き': 40,\n",
       "             '姫': 41,\n",
       "             '構築': 42,\n",
       "             '機械学習': 43,\n",
       "             '王': 44,\n",
       "             '王子': 45,\n",
       "             '男性': 46,\n",
       "             '短い': 47,\n",
       "             '自然言語処理': 48})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ベクトル化したバージョンのボキャブラリーを作成します\n",
    "TEXT.build_vocab(train_ds, vectors=japanese_fasttext_vectors, min_freq=1)\n",
    "\n",
    "# ボキャブラリーのベクトルを確認します\n",
    "print(TEXT.vocab.vectors.shape)  # 52個の単語が300次元のベクトルで表現されている\n",
    "TEXT.vocab.vectors\n",
    "\n",
    "# ボキャブラリーの単語の順番を確認します\n",
    "TEXT.vocab.stoi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女王 tensor(0.3650)\n",
      "王 tensor(0.3461)\n",
      "王子 tensor(0.5531)\n",
      "機械学習 tensor(0.0952)\n"
     ]
    }
   ],
   "source": [
    "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 姫 - 女性 + 男性\n",
    "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
    "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
    "\n",
    "# コサイン類似度を計算\n",
    "# dim=0 は0次元目で計算してくださいという指定\n",
    "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
    "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
    "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
    "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "姫 - 女性 + 男性　を計算すると狙った通り、王子がもっとも近い結果になりました"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
